{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "h_gidgX70jrm"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OoS7LtQ0lLX"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "774XsDn6lKV1"
      },
      "outputs": [],
      "source": [
        "# Read data base\n",
        "def loadDatabase(path):\n",
        "    database = open(path, 'r')\n",
        "    data = list()\n",
        "    line = database.readline()\n",
        "    while line != \"\":\n",
        "        dataSet = list(map(int, line.split()))\n",
        "        data.append(dataSet)\n",
        "        line = database.readline()\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "DOpUDIxBlQKU"
      },
      "outputs": [],
      "source": [
        "# Check if itemset is in a record\n",
        "def findInRecord(itemset, record):\n",
        "    for i in itemset:\n",
        "        if i not in record:\n",
        "            return False\n",
        "    return True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "e8MghHyvlXKU"
      },
      "outputs": [],
      "source": [
        "# Count the number of occurence of itemset in data\n",
        "def count(itemset, data):\n",
        "    counter = 0\n",
        "    for record in data:\n",
        "        if findInRecord(itemset, record):\n",
        "            counter += 1\n",
        "    return counter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfZCtuy1kr8-"
      },
      "source": [
        "# Apriori algorithm"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "```\n",
        "Let k=1\n",
        "Generate frequent itemsets of length 1\n",
        "Repeat until no new frequent itemsets are identified\n",
        "    step1: Generate length (k+1) candidate itemsets from length k frequent itemsets\n",
        "    step2: Prune candidate itemsets containing subsets of length k that are infrequent\n",
        "    step3: Count the support of each candidate by scanning the DB\n",
        "            and eliminate candidates that are infrequent, leaving only those that are frequent\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "5dg3cleBkrSS"
      },
      "outputs": [],
      "source": [
        "# merge two itemset (if possible)\n",
        "def merge(itemset1, itemset2):\n",
        "    newFreqItem = set(itemset1).union(set(itemset2))\n",
        "    if len(newFreqItem) == (len(itemset1)+1):\n",
        "        return newFreqItem\n",
        "\n",
        "\n",
        "# generate candidates of length (k+1) by merging two frequent itemsets of length k\n",
        "def generateCandidates(oldCandidates):\n",
        "    newCandidates = set()\n",
        "    for i in range(len(oldCandidates)):\n",
        "        for j in range(i + 1, len(oldCandidates)):\n",
        "            newItemset = merge(oldCandidates[i], oldCandidates[j])\n",
        "            # if merge is success, add it to newCandidate set\n",
        "            if newItemset is not None:\n",
        "                newCandidates.add(tuple(sorted(tuple(newItemset))))\n",
        "    # conver the set to a list\n",
        "    result = list()\n",
        "    for i in newCandidates:\n",
        "        result.append(sorted(i))\n",
        "    return sorted(result)\n",
        "\n",
        "\n",
        "# check whether a itemset has infrequent subsets\n",
        "def isFrequentSubset(itemset, old_candidates):\n",
        "    for i in range(len(itemset)):\n",
        "        subset = list(itemset)\n",
        "        del subset[i]\n",
        "        if subset not in old_candidates:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "\n",
        "# prune candidates which has infrequent subsets\n",
        "def prune(candidates, oldCandidates):\n",
        "    afterPrune = list()\n",
        "    for itemset in candidates:\n",
        "        if isFrequentSubset(itemset, oldCandidates):\n",
        "            afterPrune.append(itemset)\n",
        "    return afterPrune\n",
        "\n",
        "\n",
        "# generate the frequent items of length 1\n",
        "def length_1_freqItemSet(data, minsup):\n",
        "    items = dict()\n",
        "    for record in data:\n",
        "        for i in record:\n",
        "            if i in items:\n",
        "                items[i] += 1\n",
        "            else:\n",
        "                items[i] = 1\n",
        "\n",
        "    freq_items = list()\n",
        "    for item in items:\n",
        "        if items[item] >= minsup:\n",
        "            freq_items.append([[item], items[item]])\n",
        "    return sorted(freq_items)\n",
        "\n",
        "\n",
        "# data is a list of transaction, minsup is the minimum support\n",
        "def aprioriAlgorithm(data, minsup):\n",
        "    #  generate frequent itemsets of size 1\n",
        "    len_1_frq = length_1_freqItemSet(data, minsup)\n",
        "\n",
        "    freq_itemsets = list(len_1_frq)\n",
        "    candidates = []\n",
        "    for item in len_1_frq:\n",
        "        candidates.append(item[0])\n",
        "\n",
        "    while len(candidates) > 0:\n",
        "        # length K+1 candidates\n",
        "        nextCandidates = generateCandidates(candidates)\n",
        "        # remove infrequent candidates\n",
        "        nextCandidates = prune(nextCandidates, candidates)\n",
        "        # count and kick away infrequent candidate\n",
        "        candidates = list()\n",
        "        for itemSet in nextCandidates:\n",
        "            if count(itemSet, data) >= minsup:\n",
        "                candidates.append(itemSet)\n",
        "                freq_itemsets.append([itemSet, count(itemSet, data)])\n",
        "    return freq_itemsets"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "FAf8OxR_ny-l"
      },
      "source": [
        "### Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VASxPQdrnxHD",
        "outputId": "debfd0dd-a767-48bc-ee3a-3886216d4b28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length data:  2000\n",
            "Frequent itemsets:  [[[32], 290], [[38], 436], [[39], 1153], [[41], 485], [[48], 870], [[32, 39], 167], [[32, 48], 150], [[38, 39], 279], [[38, 41], 165], [[38, 48], 194], [[39, 41], 367], [[39, 48], 607], [[41, 48], 258], [[38, 39, 41], 129], [[38, 39, 48], 144], [[39, 41, 48], 204]]\n",
            "Length frequent itemsets:  16\n",
            "Time:  0.01598358154296875\n"
          ]
        }
      ],
      "source": [
        "# please refer to apriori.py for the implementation\n",
        "data = loadDatabase('a2dataset.txt')\n",
        "print(\"Length data: \", len(data))\n",
        "minsup = 100\n",
        "\n",
        "start = time.time() \n",
        "freq_itemsets = aprioriAlgorithm(data, minsup)\n",
        "print(\"Frequent itemsets: \", freq_itemsets)\n",
        "print(\"Length frequent itemsets: \", len(freq_itemsets))\n",
        "print(\"Time: \", time.time() - start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-o0cKXdwMOv"
      },
      "source": [
        "# Find closed and max itemsets with apriori algorithm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "ra1Psmb8oauT",
        "outputId": "706f39e7-1206-421f-e919-1e5a3a80087d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ClosedItemSets are: \n",
            "\n",
            "[[32], [38], [39], [41], [48], [32, 39], [32, 48], [38, 39], [41, 38], [48, 38], [41, 39], [48, 39], [48, 41], [41, 38, 39], [48, 38, 39], [48, 41, 39]]\n",
            "Length closed itemsets:  16\n",
            "\n",
            "MaxItemSets are:\n",
            "[[32, 39], [32, 48], [41, 38, 39], [48, 38, 39], [48, 41, 39]]\n",
            "Length max itemsets:  5\n",
            "Time:  0.014995336532592773\n"
          ]
        }
      ],
      "source": [
        "# check if largeSet is immediate superset of smallSet\n",
        "def isImmediateSuperSet(smallSet, largeSet):\n",
        "    if largeSet.issuperset(smallSet):\n",
        "        if len(largeSet) - len(smallSet) == 1:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "\n",
        "data = loadDatabase('a2dataset.txt')\n",
        "minsup = 100\n",
        "\n",
        "start = time.time()\n",
        "# get frequent Itemset from external library\n",
        "dataWithFreq = aprioriAlgorithm(data, minsup)\n",
        "\n",
        "freqItemSet = []\n",
        "for item in dataWithFreq:\n",
        "    freqItemSet.append(item[0])\n",
        "\n",
        "dataWithFreq = {tuple(dataWithFreq[i][0]): dataWithFreq[i][1] for i in range(len(dataWithFreq))}\n",
        "\n",
        "# find closed and max itemset\n",
        "closedItemSet = list()\n",
        "maxItemSet = list()\n",
        "\n",
        "\n",
        "# go though every freqitem set, and check if it is closed of max\n",
        "for x, y in dataWithFreq.items():\n",
        "    targetSet = set(x)\n",
        "    freq = y\n",
        "    isClosedFreqItemSet = True\n",
        "    isMaxSet = True\n",
        "    # compare a itemSet to all other itemSet\n",
        "    for p, q in dataWithFreq.items():\n",
        "        comparingSet = set(p)\n",
        "        comparingfreq = q\n",
        "        # do not do self comparison\n",
        "        if targetSet == comparingSet:\n",
        "            continue\n",
        "        if isImmediateSuperSet(targetSet, comparingSet):\n",
        "            isMaxSet = False\n",
        "            if comparingfreq == freq:\n",
        "                isClosedFreqItemSet = False\n",
        "                break\n",
        "    if isClosedFreqItemSet:\n",
        "        closedItemSet.append(list(targetSet))\n",
        "    if isMaxSet:\n",
        "        maxItemSet.append(list(targetSet))\n",
        "\n",
        "        \n",
        "print(\"ClosedItemSets are: \\n\")\n",
        "print(closedItemSet)\n",
        "print(\"Length closed itemsets: \", len(closedItemSet))\n",
        "\n",
        "print(\"\\nMaxItemSets are:\")\n",
        "print(maxItemSet)\n",
        "print(\"Length max itemsets: \", len(maxItemSet))\n",
        "\n",
        "print(\"Time: \", time.time() - start)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bjYa-OLJACfm"
      },
      "source": [
        "# Apriori algorithm is not very efficient. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ClosedItemSets are: \n",
            "\n",
            "[[32, 39], [32, 48], [41, 38, 39], [48, 38, 39], [48, 41, 39]]\n",
            "Length closed itemsets:  5\n",
            "\n",
            "MaxItemSets are:\n",
            "[[32, 39], [32, 48], [41, 38, 39], [48, 38, 39], [48, 41, 39]]\n",
            "Length max itemsets:  5\n",
            "Time:  0.015503883361816406\n"
          ]
        }
      ],
      "source": [
        "# check if largeSet is immediate superset of smallSet\n",
        "def isImmediateSuperSet(smallSet, largeSet):\n",
        "    if largeSet.issuperset(smallSet):\n",
        "        if len(largeSet) - len(smallSet) == 1:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "data = loadDatabase('a2dataset.txt')\n",
        "minsup = 100\n",
        "theta = 0.2\n",
        "\n",
        "start = time.time()\n",
        "# get frequent Itemset from external library\n",
        "dataWithFreq = aprioriAlgorithm(data, minsup)\n",
        "\n",
        "freqItemSet = []\n",
        "for item in dataWithFreq:\n",
        "    freqItemSet.append(item[0])\n",
        "\n",
        "dataWithFreq = {tuple(dataWithFreq[i][0]): dataWithFreq[i][1] for i in range(len(dataWithFreq))}\n",
        "\n",
        "# find closed and max itemset\n",
        "closedItemSet = list()\n",
        "maxItemSet = list()\n",
        "\n",
        "\n",
        "# go though every freqitem set, and check if it is closed of max\n",
        "for x, y in dataWithFreq.items():\n",
        "    targetSet = set(x)\n",
        "    freq = y\n",
        "    isClosedFreqItemSet = True\n",
        "    isMaxSet = True\n",
        "    # compare a itemSet to all other itemSet\n",
        "    for p, q in dataWithFreq.items():\n",
        "        comparingSet = set(p)\n",
        "        comparingfreq = q\n",
        "        # do not do self comparison\n",
        "        if targetSet == comparingSet:\n",
        "            continue\n",
        "        if isImmediateSuperSet(targetSet, comparingSet):\n",
        "            isMaxSet = False\n",
        "            # Compare distance patern with theta\n",
        "            if  theta <= 1 - comparingfreq / freq:\n",
        "                isClosedFreqItemSet = False\n",
        "                break\n",
        "    if isClosedFreqItemSet:\n",
        "        closedItemSet.append(list(targetSet))\n",
        "    if isMaxSet:\n",
        "        maxItemSet.append(list(targetSet))\n",
        "\n",
        "        \n",
        "print(\"ClosedItemSets are: \\n\")\n",
        "print(closedItemSet)\n",
        "print(\"Length closed itemsets: \", len(closedItemSet))\n",
        "\n",
        "print(\"\\nMaxItemSets are:\")\n",
        "print(maxItemSet)\n",
        "print(\"Length max itemsets: \", len(maxItemSet))\n",
        "\n",
        "print(\"Time: \", time.time() - start)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Redundancy-aware top-k patterns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "def calculate_redundancy(data, centroid, data_point, min_default=50):\n",
        "    # print(centroid)\n",
        "    # R(p,q) = S(p) + S(q) âˆ’ S(p,q)\n",
        "    new_itemsets = list(data.keys())[centroid] + list(data.keys())[data_point]\n",
        "    new_itemsets = tuple(set(new_itemsets))\n",
        "    new_itemsets = tuple(sorted(new_itemsets))\n",
        "    \n",
        "    S_p_q = min_default if new_itemsets not in data else data[new_itemsets]\n",
        "    S_p = data[list(data.keys())[centroid]]\n",
        "    S_q = data[list(data.keys())[data_point]]\n",
        "\n",
        "    return S_p + S_q - S_p_q\n",
        "    # return S_p_q\n",
        "\n",
        "def get_centroid(data, groupdata, old_centroid):\n",
        "    min_red = 9999999999999\n",
        "    min_centroid = old_centroid\n",
        "    for centroid in groupdata:\n",
        "        red = 0\n",
        "        for datapoint in groupdata:\n",
        "            red = red + calculate_redundancy(data, centroid, datapoint)\n",
        "        if red < min_red:\n",
        "            min_centroid = centroid\n",
        "\n",
        "    return min_centroid\n",
        "\n",
        "class TopK:\n",
        "    def __init__(self, tol = 0.0001, max_iter = 300) -> None:\n",
        "        self.tol = tol\n",
        "        self.max_iter = max_iter\n",
        "    \n",
        "    def fit(self, data, k=2):\n",
        "        self.centroids = {}\n",
        "        data_ids = np.arange(len(data))\n",
        "        data_keys = data.keys()\n",
        "        cache_ = []\n",
        "\n",
        "        for i in range(k):\n",
        "            while True:\n",
        "                chosen_id = random.randint(0, len(data)-1)\n",
        "                if chosen_id not in cache_:\n",
        "                    cache_.append(chosen_id)\n",
        "                    self.centroids[i] = data_ids[chosen_id]\n",
        "                    break\n",
        "\n",
        "\n",
        "        for iter in range(self.max_iter):\n",
        "            self.groupdata = {}\n",
        "            for i in range(k):\n",
        "                self.groupdata[i] = []\n",
        "\n",
        "            for data_point in data_ids:\n",
        "                min_dist, min_centroid = 9999999, None\n",
        "                for j, centroid in self.centroids.items(): \n",
        "                    # for each centroid\n",
        "                    dist = calculate_redundancy(data, centroid, data_point)\n",
        "                    if dist < min_dist:\n",
        "                        min_dist = dist\n",
        "                        min_centroid = j\n",
        "                index = min_centroid\n",
        "\n",
        "                self.groupdata[index].append(data_point)\n",
        "            \n",
        "            prev_centroids = dict(self.centroids)\n",
        "            optimized = True\n",
        "\n",
        "            for index in self.centroids:\n",
        "                self.centroids[index] = get_centroid(data, self.groupdata[index], self.centroids[index])\n",
        "\n",
        "            for i in self.centroids:\n",
        "                original_centroid = prev_centroids[i]\n",
        "                current_centroid = self.centroids[i]\n",
        "                if (np.sum((current_centroid - original_centroid)/original_centroid * 1000.0) > self.tol):\n",
        "                    optimized = False\n",
        "            \n",
        "            if optimized:\n",
        "                break\n",
        "        \n",
        "    def predict(self, data):\n",
        "        data_point = data\n",
        "\n",
        "        min_dist, min_centroid = 9999999, None\n",
        "        for j, centroid in self.centroids.items(): \n",
        "            # for each centroid\n",
        "            dist = calculate_redundancy(data, centroid, data_point)\n",
        "            if dist < min_dist:\n",
        "                min_dist = dist\n",
        "                min_centroid = j\n",
        "        index = min_centroid\n",
        "        return index, self.centroids[index]\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    k = 5\n",
        "    model = TopK()\n",
        "    model.fit(dataWithFreq, k=k)\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 --- (38, 39, 48)\n",
            "1 --- (38,)\n",
            "2 --- (39, 41)\n",
            "3 --- (39, 41, 48)\n",
            "4 --- (41, 48)\n"
          ]
        }
      ],
      "source": [
        "for key, data_point in model.centroids.items():\n",
        "    print(key, \"---\", list(dataWithFreq.keys())[data_point])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.11"
    },
    "vscode": {
      "interpreter": {
        "hash": "317ad386929a8fddfdac9f879dda3284d1ccf3cc89d61bc50c41f8e40366abd8"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
